{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 层和快"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 事实证明，研究讨论“比单个层大”但“比整个模型小”的组件更有价值。\n",
    "# 在计算机视觉中广泛流行的ResNet-152架构就有数百层， 这些层是由层组（groups of layers）的重复模式组成。\n",
    "\n",
    "# 为了实现这些复杂的网络，我们引入了神经网络块的概念。 块（block）可以描述单个层、由多个层组成的组件或整个模型本身。 \n",
    "# 使用块进行抽象的一个好处是可以将一些块组合成更大的组件， 这一过程通常是递归的。\n",
    "\n",
    "# 从编程的角度来看，块由类（class）表示。 它的任何子类都必须定义一个将其输入转换为输出的前向传播函数， 并且必须存储任何必需的参数。 \n",
    "# 注意，有些块不需要任何参数。 最后，为了计算梯度，块必须具有反向传播函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0410, -0.1372,  0.0810, -0.1239, -0.0334, -0.0479,  0.0713, -0.0861,\n",
       "         -0.2081,  0.1387],\n",
       "        [-0.0695, -0.1340,  0.1005, -0.0411,  0.0583, -0.0846, -0.0025, -0.1396,\n",
       "         -0.2535,  0.1863]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "x = torch.rand(2, 20)\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过实例化nn.Sequential来构建我们的模型， 层的执行顺序是作为参数传递的。 \n",
    "# 简而言之，nn.Sequential定义了一种特殊的Module， 即在PyTorch中表示一个块的类， 它维护了一个由Module组成的有序列表。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.1 自定义块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 总结一下每个块必须提供的基本功能。\n",
    "# 将输入数据作为其前向传播函数的参数。\n",
    "# 通过前向传播函数来生成输出。请注意，输出的形状可能与输入的形状不同。\n",
    "# 计算其输出关于输入的梯度，可通过其反向传播函数进行访问。通常这是自动发生的。\n",
    "# 存储和访问前向传播计算所需的参数。\n",
    "# 根据需要初始化模型参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        # 调用父类构造方法\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)\n",
    "        self.out = nn.Linear(256, 10)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.out(F.relu(self.hidden(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先，我们定制的__init__函数通过super().__init__() 调用父类的__init__函数， 省去了重复编写模版代码的痛苦。 \n",
    "# 然后，我们实例化两个全连接层， 分别为self.hidden和self.out。 \n",
    "# 注意，除非我们实现一个新的运算符， 否则我们不必担心反向传播函数或参数初始化， 系统将自动生成这些。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1951,  0.0603, -0.0150,  0.0168, -0.0388, -0.0008, -0.0169,  0.2181,\n",
       "         -0.0879, -0.0889],\n",
       "        [ 0.1181, -0.0842,  0.0618,  0.0164, -0.1001,  0.2193,  0.0725,  0.2883,\n",
       "         -0.1299, -0.0810]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 块的一个主要优点是它的多功能性。 \n",
    "# 我们可以子类化块以创建层（如全连接层的类）、 整个模型（如上面的MLP类）或具有中等复杂度的各种组件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.2 顺序块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential的设计是为了把其他模块串起来。 为了构建我们自己的简化的MySequential， 我们只需要定义两个关键函数：\n",
    "# 一种将块逐个追加到列表中的函数；\n",
    "# 一种前向传播函数，用于将输入按追加块的顺序传递给块组成的“链条”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            # 变量_modules中。_module的类型是OrderedDict\n",
    "            self._modules[idx] = module\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # OrderedDict保证了按照成员添加的顺序遍历它们\n",
    "        for block in self._modules.values():\n",
    "            X = block(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __init__函数将每个模块逐个添加到有序字典_modules中。\n",
    "# 简而言之，_modules的主要优点是： 在模块的参数初始化过程中， 系统知道在_modules字典中查找需要初始化参数的子块。\n",
    "# 当MySequential的前向传播函数被调用时， 每个添加的块都按照它们被添加的顺序执行。 现在可以使用我们的MySequential类重新实现多层感知机。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1425,  0.0310,  0.0660,  0.2599,  0.2230, -0.3800,  0.0844,  0.0538,\n",
       "          0.2024,  0.0669],\n",
       "        [-0.1793, -0.1722,  0.0018,  0.1016,  0.2633, -0.3675, -0.1107,  0.1252,\n",
       "          0.1825, -0.0038]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.3 在前向传播函数中执行代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rand_weight = torch.rand((20, 20), requires_grad=False)\n",
    "        self.linear = nn.Linear(20, 20)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        X = F.relu(torch.mm(X, self.rand_weight))\n",
    "        X = self.linear(X)\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在这个FixedHiddenMLP模型中，我们实现了一个隐藏层， 其权重（self.rand_weight）在实例化时被随机初始化，之后为常量。 \n",
    "# 这个权重不是一个模型参数，因此它永远不会被反向传播更新。 \n",
    "# 然后，神经网络将这个固定层的输出通过一个全连接层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0344, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = FixedHiddenMLP()\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可以混合搭配各种组合块的方法。 在下面的例子中，我们以一些想到的方法嵌套块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0544, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self, net):\n",
    "        super().__init__()\n",
    "        self.net = net()\n",
    "        self.linear = nn.Linear(10, 2)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.linear(F.relu(self.net(X)))\n",
    "    \n",
    "net = nn.Sequential(NestMLP(MLP), nn.Linear(2, 20), nn.ReLU(), FixedHiddenMLP())\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 小结\n",
    "# 一个块可以由许多层组成；一个块可以由许多块组成。\n",
    "# 块可以包含代码。\n",
    "# 块负责大量的内部处理，包括参数初始化和反向传播。\n",
    "# 层和块的顺序连接由Sequential块处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2 参数管理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 此外，有时我们希望提取参数，以便在其他环境中复用它们， 将模型保存下来，以便它可以在其他软件中执行， 或者为了获得科学的理解而进行检查。\n",
    "\n",
    "# 之前的介绍中，我们只依靠深度学习框架来完成训练的工作， 而忽略了操作参数的具体细节。 本节，我们将介绍以下内容：\n",
    "# 访问参数，用于调试、诊断和可视化；\n",
    "# 参数初始化；\n",
    "# 在不同模型组件间共享参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0247],\n",
       "        [-0.1735]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))\n",
    "x = torch.rand(size=(2, 4))\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.1 参数访问"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 当通过Sequential类定义模型时， 我们可以通过索引来访问模型的任意层。 这就像模型是一个列表一样，每层的参数都在其属性中。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Linear(in_features=4, out_features=8, bias=True),\n",
       " OrderedDict([('weight',\n",
       "               tensor([[ 0.3940, -0.4682, -0.0484, -0.4514],\n",
       "                       [ 0.0345, -0.1127,  0.2377,  0.0185],\n",
       "                       [-0.3083, -0.0885,  0.2116,  0.3757],\n",
       "                       [ 0.4567,  0.2557, -0.4169, -0.4126],\n",
       "                       [-0.1336, -0.0068, -0.4028, -0.4020],\n",
       "                       [ 0.0239,  0.2496, -0.1452, -0.2991],\n",
       "                       [ 0.2593,  0.3968, -0.2789,  0.4879],\n",
       "                       [ 0.4644,  0.4868,  0.2633,  0.1321]])),\n",
       "              ('bias',\n",
       "               tensor([-0.2622,  0.4982, -0.0424,  0.2982, -0.4474,  0.3968, -0.3667, -0.3732]))]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0], net[0].state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先，这个全连接层包含两个参数，分别是该层的权重和偏置。 两者都存储为单精度浮点数（float32）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1.1 目标参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.nn.parameter.Parameter,\n",
       " Parameter containing:\n",
       " tensor([-0.1317], requires_grad=True),\n",
       " tensor([-0.1317]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 每个参数都表示为参数类的一个实例。 要对参数执行任何操作，首先我们需要访问底层的数值。\n",
    "# 从第二个全连接层（即第三个神经网络层）提取偏置， 提取后返回的是一个参数类实例，并进一步访问该参数的值。\n",
    "type(net[2].bias), net[2].bias, net[2].bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[ 0.3940, -0.4682, -0.0484, -0.4514],\n",
       "         [ 0.0345, -0.1127,  0.2377,  0.0185],\n",
       "         [-0.3083, -0.0885,  0.2116,  0.3757],\n",
       "         [ 0.4567,  0.2557, -0.4169, -0.4126],\n",
       "         [-0.1336, -0.0068, -0.4028, -0.4020],\n",
       "         [ 0.0239,  0.2496, -0.1452, -0.2991],\n",
       "         [ 0.2593,  0.3968, -0.2789,  0.4879],\n",
       "         [ 0.4644,  0.4868,  0.2633,  0.1321]], requires_grad=True),\n",
       " tensor([[ 0.3940, -0.4682, -0.0484, -0.4514],\n",
       "         [ 0.0345, -0.1127,  0.2377,  0.0185],\n",
       "         [-0.3083, -0.0885,  0.2116,  0.3757],\n",
       "         [ 0.4567,  0.2557, -0.4169, -0.4126],\n",
       "         [-0.1336, -0.0068, -0.4028, -0.4020],\n",
       "         [ 0.0239,  0.2496, -0.1452, -0.2991],\n",
       "         [ 0.2593,  0.3968, -0.2789,  0.4879],\n",
       "         [ 0.4644,  0.4868,  0.2633,  0.1321]]),\n",
       " True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 参数是复合的对象，包含值、梯度和额外信息。 这就是我们需要显式参数值的原因。 \n",
    "# 除了值之外，我们还可以访问每个参数的梯度。 在上面这个网络中，由于我们还没有调用反向传播，所以参数的梯度处于初始状态。\n",
    "net[0].weight, net[0].weight.data, net[0].weight.grad == None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1.2 一次性访问所有参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generator"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(net[0].named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n",
      "('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))\n"
     ]
    }
   ],
   "source": [
    "# \"*\"是一个特殊的操作符，称为“解包”运算符（unpacking operator）。它可以将一个可迭代对象（如列表、元组等）中的元素解包成单独的参数。\n",
    "print(*[(name, param.shape)for name, param in net[0].named_parameters()])\n",
    "print(*[(name, param.shape) for name, param in net.named_parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(OrderedDict([('weight',\n",
       "               tensor([[ 0.3940, -0.4682, -0.0484, -0.4514],\n",
       "                       [ 0.0345, -0.1127,  0.2377,  0.0185],\n",
       "                       [-0.3083, -0.0885,  0.2116,  0.3757],\n",
       "                       [ 0.4567,  0.2557, -0.4169, -0.4126],\n",
       "                       [-0.1336, -0.0068, -0.4028, -0.4020],\n",
       "                       [ 0.0239,  0.2496, -0.1452, -0.2991],\n",
       "                       [ 0.2593,  0.3968, -0.2789,  0.4879],\n",
       "                       [ 0.4644,  0.4868,  0.2633,  0.1321]])),\n",
       "              ('bias',\n",
       "               tensor([-0.2622,  0.4982, -0.0424,  0.2982, -0.4474,  0.3968, -0.3667, -0.3732]))]),\n",
       " OrderedDict([('0.weight',\n",
       "               tensor([[ 0.3940, -0.4682, -0.0484, -0.4514],\n",
       "                       [ 0.0345, -0.1127,  0.2377,  0.0185],\n",
       "                       [-0.3083, -0.0885,  0.2116,  0.3757],\n",
       "                       [ 0.4567,  0.2557, -0.4169, -0.4126],\n",
       "                       [-0.1336, -0.0068, -0.4028, -0.4020],\n",
       "                       [ 0.0239,  0.2496, -0.1452, -0.2991],\n",
       "                       [ 0.2593,  0.3968, -0.2789,  0.4879],\n",
       "                       [ 0.4644,  0.4868,  0.2633,  0.1321]])),\n",
       "              ('0.bias',\n",
       "               tensor([-0.2622,  0.4982, -0.0424,  0.2982, -0.4474,  0.3968, -0.3667, -0.3732])),\n",
       "              ('2.weight',\n",
       "               tensor([[-0.3520,  0.2372,  0.1174, -0.3491,  0.2054, -0.0834, -0.0009, -0.0541]])),\n",
       "              ('2.bias', tensor([-0.1317]))]),\n",
       " tensor([-0.1317]),\n",
       " collections.OrderedDict)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].state_dict(), net.state_dict(), net.state_dict()[\"2.bias\"], type(net.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1.3 从嵌套块收集参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0690],\n",
       "        [-0.0690]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def block1():\n",
    "    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 4), nn.ReLU())\n",
    "\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    for i in range(4):\n",
    "        net.add_module(f'block {i}', block1())\n",
    "    return net\n",
    "\n",
    "net = nn.Sequential(block2(), nn.Linear(4, 1))\n",
    "\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (block 0): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 1): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 2): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 3): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (1): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(OrderedDict([('weight',\n",
       "               tensor([[ 0.2441, -0.0478,  0.1856, -0.0560,  0.3405, -0.0802, -0.0579, -0.2285],\n",
       "                       [ 0.2837,  0.0108,  0.0430, -0.2924, -0.3194,  0.2788,  0.3437,  0.3407],\n",
       "                       [-0.1011,  0.2079, -0.0312,  0.1886,  0.2244,  0.0808, -0.2279, -0.0587],\n",
       "                       [ 0.1158, -0.3045, -0.0939,  0.0132, -0.1913,  0.0974,  0.1338,  0.1098]])),\n",
       "              ('bias', tensor([-0.0556, -0.1877, -0.3186, -0.2497]))]),\n",
       " tensor([-0.0556, -0.1877, -0.3186, -0.2497]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 因为层是分层嵌套的，所以我们也可以像通过嵌套列表索引一样访问它们。\n",
    "net[0][0][2].state_dict(), net[0][0][2].bias.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.2 参数初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 深度学习框架提供默认随机初始化， 也允许我们创建自定义初始化方法， 满足我们通过其他规则实现初始化权重。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2.1 内置初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1., 1.]]), tensor([0.]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将所有权重参数初始化为标准差为0.01的高斯随机变量， 且将偏置参数设置为0。\n",
    "# 还可以将所有参数初始化为给定的常数，比如初始化为1。\n",
    "\n",
    "def init_normal(m):\n",
    "    if type(m) == nn.Linear:\n",
    "#         nn.init.normal_(m.weight, mean=0, std=0.1)\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.zeros_(m.bias)\n",
    "net.apply(init_normal)\n",
    "\n",
    "net[1].weight.data, net[1].bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.4261, -0.4087, -0.1310, -0.6509,  0.0934, -0.2760, -0.2651, -0.2921],\n",
       "         [ 0.2366,  0.0433, -0.6155,  0.4806,  0.6806, -0.3063, -0.4704, -0.5921],\n",
       "         [ 0.5900, -0.3982, -0.2367,  0.2896, -0.4423, -0.6251, -0.5095,  0.6011],\n",
       "         [ 0.1074,  0.6398, -0.0424,  0.5191,  0.0122,  0.4108,  0.2883, -0.1933]]),\n",
       " tensor([[42., 42., 42., 42.]]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 还可以对某些块应用不同的初始化方法\n",
    "def init_xavier(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "def init_42(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 42)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "net[0][0][2].apply(init_xavier)\n",
    "net[1].apply(init_42)\n",
    "net[0][0][2].weight.data, net[1].weight.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 5.2.2.2 自定义初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init: ('weight', torch.Size([4, 8])) ('bias', torch.Size([4]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-6.9457, -5.5220,  0.0000, -0.0000, -7.9457, -8.2565,  7.5487, -0.0000],\n",
       "        [-5.0342, -5.5311, -0.0000, -0.0000, -0.0000, -6.2623,  7.9626, -7.8817],\n",
       "        [-0.0000, -0.0000,  7.3876,  8.5117,  6.5371,  0.0000, -7.5257,  0.0000],\n",
       "        [ 0.0000, -0.0000, -6.2121, -6.4043,  5.2628,  6.7303,  0.0000, -7.3472]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        # [0]比*优先级高\n",
    "        print(\"init:\", *[(name, param.shape) for name, param in m.named_parameters()])\n",
    "        nn.init.uniform_(m.weight, -10, 10)\n",
    "        m.weight.data *= m.weight.data.abs() >= 5.0\n",
    "\n",
    "net[0][0][2].apply(my_init)\n",
    "net[0][0][2].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[42.0000, -4.5220,  1.0000,  1.0000, -6.9457, -7.2565,  8.5487,  1.0000],\n",
       "        [-4.0342, -4.5311,  1.0000,  1.0000,  1.0000, -5.2623,  8.9626, -6.8817],\n",
       "        [ 1.0000,  1.0000,  8.3876,  9.5117,  7.5371,  1.0000, -6.5257,  1.0000],\n",
       "        [ 1.0000,  1.0000, -5.2121, -5.4043,  6.2628,  7.7303,  1.0000, -6.3472]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 始终可以直接设置参数，必须是浮点数\n",
    "net[0][0][2].weight.data[:] += 1.0\n",
    "net[0][0][2].weight.data[0, 0] = 42.0\n",
    "net[0][0][2].weight.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.3 参数绑定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 有时我们希望在多个层间共享参数： 我们可以定义一个稠密层，然后使用它的参数来设置另一个层的参数\n",
    "# 当参数绑定时，梯度会发生什么情况？ 答案是由于模型参数包含梯度，\n",
    "# 因此在反向传播期间第二个隐藏层 （即第三个神经网络层）和第三个隐藏层（即第五个神经网络层）的梯度会加在一起。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shared = nn.Linear(8, 8)\n",
    "\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), \n",
    "                    shared, nn.ReLU(), \n",
    "                    shared, nn.ReLU(), \n",
    "                    nn.Linear(8, 1))\n",
    "\n",
    "print(net[2].weight.data == net[4].weight.data)\n",
    "net[2].weight.data[0, 0] == 100\n",
    "net[2].weight.data[0, 0] == net[4].weight.data[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 小结\n",
    "# 有几种方法可以访问、初始化和绑定模型参数。\n",
    "# named_parameters(), weight/bias.data\n",
    "# 可以使用自定义初始化方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 延后初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们忽略了建立网络时需要做的以下这些事情：\n",
    "# 我们定义了网络架构，但没有指定输入维度。\n",
    "# 我们添加层时没有指定前一层的输出维度。\n",
    "# 我们在初始化参数时，甚至没有足够的信息来确定模型应该包含多少参数。\n",
    "\n",
    "# 框架的延后初始化（defers initialization）， 即直到数据第一次通过模型传递时，框架才会动态地推断出每个层的大小。\n",
    "# 现在我们在编写代码时无须知道维度是什么就可以设置参数， 这种能力可以大大简化定义和修改模型的任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(OrderedDict([('0.weight',\n",
       "               tensor([[-0.0922, -0.2700,  0.2611, -0.3351,  0.0165,  0.0603,  0.2575, -0.0633],\n",
       "                       [-0.1497,  0.1818, -0.2349,  0.0594,  0.0064, -0.1249, -0.1899,  0.0062],\n",
       "                       [-0.3051, -0.0467, -0.0930,  0.0106,  0.2789, -0.0088,  0.1491,  0.1987],\n",
       "                       [-0.1669, -0.2807,  0.2471, -0.1017,  0.0212, -0.2038,  0.1812, -0.0802]])),\n",
       "              ('0.bias', tensor([ 0.3053,  0.0416, -0.0666,  0.2319])),\n",
       "              ('2.weight', tensor([[-0.0808,  0.2645, -0.2859, -0.0334]])),\n",
       "              ('2.bias', tensor([0.1155]))]),\n",
       " <generator object Module.named_parameters at 0x0000012363E7DAC0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pytorch 是在实例化网络之前就自动推断的网络结构的\n",
    "net = nn.Sequential(nn.Linear(8, 4), nn.ReLU(), \n",
    "                    nn.Linear(4, 1))\n",
    "net.state_dict(),net.named_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一旦我们知道输入维数是20，框架可以通过代入值20来识别第一层权重矩阵的形状。 \n",
    "# 识别出第一层的形状后，框架处理第二层，依此类推，直到所有形状都已知为止。 \n",
    "# 注意，在这种情况下，只有第一层需要延迟初始化，但是框架仍是按顺序初始化的。 等到知道了所有的参数形状，框架就可以初始化参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 小结\n",
    "# 延后初始化使框架能够自动推断参数形状，使修改模型架构变得容易，避免了一些常见的错误。\n",
    "# 我们可以通过模型传递数据，使框架最终初始化参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 自定义层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 有时我们会遇到或要自己发明一个现在在深度学习框架中还不存在的层。 在这些情况下，必须构建自定义层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.1 不带参数的层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面的CenteredLayer类要从其输入中减去均值。 要构建它，我们只需继承基础层类并实现前向传播功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CenteredLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return X - X.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2., -1.,  0.,  1.,  2.])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = CenteredLayer()\n",
    "x = torch.FloatTensor([1, 2, 3, 4, 5])\n",
    "layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.1176e-08, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(layer, nn.Linear(8, 4), nn.ReLU(), nn.Linear(4, 1), layer)\n",
    "net(torch.rand(4, 8)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.2 带参数的层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 继续定义具有参数的层， 这些参数可以通过训练进行调整。 我们可以使用内置函数来创建参数，这些函数提供一些基本的管理功能。 \n",
    "# 比如管理访问、初始化、共享、保存和加载模型参数。 这样做的好处之一是：我们不需要为每个自定义层编写自定义的序列化程序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinear(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.rand(num_inputs, num_outputs))\n",
    "        self.bias = nn.Parameter(torch.rand(num_outputs))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return F.relu(torch.matmul(X, self.weight) + self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.3701, 1.4377, 1.8227],\n",
       "         [2.1141, 2.3025, 2.2817]], grad_fn=<ReluBackward0>),\n",
       " Parameter containing:\n",
       " tensor([[0.3173, 0.3272, 0.9134],\n",
       "         [0.2066, 0.4206, 0.7424],\n",
       "         [0.4654, 0.5302, 0.1476],\n",
       "         [0.1768, 0.1229, 0.3049],\n",
       "         [0.5746, 0.7111, 0.2343]], requires_grad=True))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = MyLinear(5, 3)\n",
    "linear(torch.rand(2, 5)), linear.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-53.6350],\n",
       "         [-51.4538]], grad_fn=<AddmmBackward0>),\n",
       " OrderedDict([('weight',\n",
       "               tensor([[0.2091, 0.3200, 0.1484, 0.6543],\n",
       "                       [0.0115, 0.0439, 0.2171, 0.7234],\n",
       "                       [0.6916, 0.3178, 0.0672, 0.4371],\n",
       "                       [0.8583, 0.3327, 0.6265, 0.8974],\n",
       "                       [0.1274, 0.1681, 0.4258, 0.4720],\n",
       "                       [0.8530, 0.9966, 0.3755, 0.8795],\n",
       "                       [0.9606, 0.8680, 0.0230, 0.6018],\n",
       "                       [0.6324, 0.4868, 0.5589, 0.8270]])),\n",
       "              ('bias', tensor([0.6321, 0.7701, 0.5923, 0.0471]))]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可以使用自定义层构建模型，就像使用内置的全连接层一样使用自定义层。\n",
    "net = nn.Sequential(MyLinear(64, 8), MyLinear(8, 4), nn.Linear(4, 1))\n",
    "net(torch.rand(2, 64)), net[1].state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 小结\n",
    "# 我们可以通过基本层类设计自定义层。这允许我们定义灵活的新层，其行为与深度学习框架中的任何现有层不同。\n",
    "# 在自定义层定义完成后，我们就可以在任意环境和网络架构中调用该自定义层。\n",
    "# 层可以有局部参数，这些参数可以通过内置函数创建。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.5 读写文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 有时我们希望保存训练的模型， 以备将来在各种环境中使用（比如在部署中进行预测）。 \n",
    "# 此外，当运行一个耗时较长的训练过程时， 最佳的做法是定期保存中间结果， 以确保在服务器电源被不小心断掉时，我们不会损失几天的计算结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5.1 加载和保存张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对于单个张量，我们可以直接调用load和save函数分别读写它们。 这两个函数都要求我们提供一个名称，save要求将要保存的变量作为输入。\n",
    "\n",
    "x = torch.arange(4)\n",
    "torch.save(x, 'x-file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = torch.load('x-file')\n",
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.normal(0, 1, size=(4, 4))\n",
    "torch.save([x, y], 'x-files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3]),\n",
       " tensor([[-1.9566, -1.2968, -0.3224, -1.5092],\n",
       "         [-0.4728, -1.9377,  0.3600, -1.0220],\n",
       "         [ 0.0944,  0.5364,  0.7123, -0.3216],\n",
       "         [-0.4736,  0.3793,  2.2344, -0.5410]]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可以存储一个张量列表，然后把它们读回内存。\n",
    "x2, y2 = torch.load('x-files')\n",
    "x2, y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([0, 1, 2, 3]),\n",
       " 'y': tensor([[-1.9566, -1.2968, -0.3224, -1.5092],\n",
       "         [-0.4728, -1.9377,  0.3600, -1.0220],\n",
       "         [ 0.0944,  0.5364,  0.7123, -0.3216],\n",
       "         [-0.4736,  0.3793,  2.2344, -0.5410]])}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可以写入或读取从字符串映射到张量的字典。 当我们要读取或写入模型中的所有权重时，这很方便。\n",
    "mydict = {'x': x, 'y': y}\n",
    "torch.save(mydict, 'mydict')\n",
    "mydict2 = torch.load('mydict')\n",
    "mydict2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5.2 加载和保存模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 深度学习框架提供了内置函数来保存和加载整个网络。\n",
    "# 需要注意的一个重要细节是，这将保存模型的参数而不是保存整个模型。 \n",
    "# 需要注意的一个重要细节是，这将保存模型的参数而不是保存整个模型。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)\n",
    "        self.out = nn.Linear(256, 10)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.out(F.relu(self.hidden(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2361,  0.0662, -0.1071, -0.0196, -0.1741, -0.2000,  0.0759,  0.0711,\n",
       "         -0.0472,  0.0370],\n",
       "        [-0.2777,  0.0955, -0.2173, -0.0160, -0.1170, -0.2941,  0.1579,  0.0574,\n",
       "         -0.1415, -0.0962]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()\n",
    "x = torch.rand(2, 20)\n",
    "y = net(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型参数\n",
    "torch.save(net.state_dict(), 'mlp.params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[True, True, True, True, True, True, True, True, True, True],\n",
       "         [True, True, True, True, True, True, True, True, True, True]]),\n",
       " MLP(\n",
       "   (hidden): Linear(in_features=20, out_features=256, bias=True)\n",
       "   (out): Linear(in_features=256, out_features=10, bias=True)\n",
       " ))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 实例化了原始多层感知机模型的一个备份。 这里我们不需要随机初始化模型参数，而是直接读取文件中存储的参数。\n",
    "clone = MLP()\n",
    "clone.load_state_dict(torch.load('mlp.params'))\n",
    "y2 = clone(x)\n",
    "y2 == y, clone.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 小结\n",
    "# save和load函数可用于张量对象的文件读写。\n",
    "# 我们可以通过参数字典保存和加载网络的全部参数。\n",
    "# 保存架构必须在代码中完成，而不是在参数中完成。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.6 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如何使用单个GPU，然后是如何使用多个GPU和多个服务器（具有多个GPU）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr 18 22:34:36 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 527.99       Driver Version: 527.99       CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   40C    P8     1W /  95W |   2378MiB /  6141MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     12020      C   ...conda\\envs\\d2l\\python.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# !nvidia-smi 是 Jupyter Notebook 或者类似环境下的一种魔法命令，用于显示当前 NVIDIA GPU 设备的状态信息。该命令在终端或者命令行界面下是无效的。\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在PyTorch中，每个数组都有一个设备（device）， 我们通常将其称为环境（context）。 默认情况下，所有变量和相关的计算都分配给CPU。\n",
    "# 有时环境可能是GPU。 当我们跨多个服务器部署作业时，事情会变得更加棘手。\n",
    "# 通过智能地将数组分配给环境， 我们可以最大限度地减少在设备之间传输数据的时间。 \n",
    "# 例如，当在带有GPU的服务器上训练神经网络时， 我们通常希望模型的参数在GPU上。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6.1 计算设备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cpu'), device(type='cuda'), device(type='cuda', index=0))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在PyTorch中，CPU和GPU可以用torch.device('cpu') 和torch.device('cuda')表示。 \n",
    "# 应该注意的是，cpu设备意味着所有物理CPU和内存， 这意味着PyTorch的计算将尝试使用所有CPU核心。 \n",
    "# 然而，gpu设备只代表一个卡和相应的显存。 如果有多个GPU，我们使用torch.device(f'cuda:{i}') 来表示第\n",
    "# i块GPU（从0开始）。 另外，cuda:0和cuda是等价的。\n",
    "torch.device('cpu'), torch.device('cuda'), torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查询可用gpu的数量。\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_gpu(i=0):\n",
    "    \"\"\"有cuda i返回gpu i，否则返回cpu\"\"\"\n",
    "    if torch.cuda.device_count() >= i + 1:\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    return torch.device('cpu')\n",
    "\n",
    "def try_all_gpus():\n",
    "    \"\"\"返回所有可用的gpu\"\"\"\n",
    "    devices = [torch.device(f'cuda:{i}') for i in range(torch.cuda.device_count())]\n",
    "    return devices if devices else [torch.device('cpu')]\n",
    "\n",
    "# 是的，if devices是在判断devices列表是否为空。\n",
    "# 当devices列表为空时，条件判断结果为False，因此会执行后面的[torch.device('cpu')]语句，返回只包含CPU设备的列表。\n",
    "# 而当devices列表不为空时，条件判断结果为True，因此会直接返回devices列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cuda', index=0),\n",
       " device(type='cpu'),\n",
       " [device(type='cuda', index=0)])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try_gpu(), try_gpu(10), try_all_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6.2 张量与GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可以查询张量所在的设备。 默认情况下，张量是在CPU上创建的。\n",
    "# 需要注意的是，无论何时我们要对多个项进行操作， 它们都必须在同一个设备上。 \n",
    "# 例如，如果我们对两个张量求和， 我们需要确保两个张量都位于同一个设备上， 否则框架将不知道在哪里存储结果，甚至不知道在哪里执行计算。\n",
    "x = torch.normal(0, 1, size=(4, 4))\n",
    "x.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.2.1 存储在GPU上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]], device='cuda:0')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 有几种方法可以在GPU上存储张量。 例如，我们可以在创建张量时指定存储设备。\n",
    "# 在GPU上创建的张量只消耗这个GPU的显存。 我们可以使用nvidia-smi命令查看显存使用情况。 一般来说，我们需要确保不创建超过GPU显存限制的数据。\n",
    "\n",
    "x = torch.ones(3, 3, device=try_gpu())\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.6938, 0.4960, 0.6222],\n",
       "         [0.2703, 0.1311, 0.2305],\n",
       "         [0.9875, 0.9919, 0.2680]]),\n",
       " device(type='cpu'))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 假设我们至少有两个GPU，下面的代码将在第二个GPU上创建一个随机张量。\n",
    "y = torch.rand(3, 3, device=try_gpu(1))\n",
    "y, y.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.2.2 复制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]], device='cuda:0'),\n",
       " tensor([[0.6938, 0.4960, 0.6222],\n",
       "         [0.2703, 0.1311, 0.2305],\n",
       "         [0.9875, 0.9919, 0.2680]]),\n",
       " tensor([[0.6938, 0.4960, 0.6222],\n",
       "         [0.2703, 0.1311, 0.2305],\n",
       "         [0.9875, 0.9919, 0.2680]], device='cuda:0'))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 如果我们要计算X + Y，我们需要决定在哪里执行这个操作。 \n",
    "# 例如，可以将X传输到第二个GPU并在那里执行操作。 \n",
    "# 不要简单地X加上Y，因为这会导致异常， 运行时引擎不知道该怎么做：它在同一设备上找不到数据会导致失败。 \n",
    "# 由于Y位于第二个GPU上，所以我们需要将X移到那里， 然后才能执行相加运算。\n",
    "z = y.cuda(0)\n",
    "x, y, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.6938, 1.4960, 1.6222],\n",
       "        [1.2703, 1.1311, 1.2305],\n",
       "        [1.9875, 1.9919, 1.2680]], device='cuda:0')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x + z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 假设变量Z已经存在于第二个GPU上。 如果我们还是调用Z.cuda(1)会发生什么？ 它将返回Z，而不会复制并分配新内存。\n",
    "z.cuda(0) is z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 人们使用GPU来进行机器学习，因为单个GPU相对运行速度快。 但是在设备（CPU、GPU和其他机器）之间传输数据比计算慢得多。 \n",
    "# 这也使得并行化变得更加困难，因为我们必须等待数据被发送（或者接收）， 然后才能继续进行更多的操作。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6.3 神经网络与GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4209],\n",
       "        [-1.4209],\n",
       "        [-1.4209]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 类似地，神经网络模型可以指定设备。 下面的代码将模型参数放在GPU上。\n",
    "\n",
    "net = nn.Sequential(nn.Linear(3, 1))\n",
    "net.to(device=try_gpu())\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight',\n",
       "              tensor([[-0.3763, -0.0095, -0.5741]], device='cuda:0')),\n",
       "             ('0.bias', tensor([-0.4611], device='cuda:0'))])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 总之，只要所有的数据和参数都在同一个设备上， 我们就可以有效地学习模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 小结\n",
    "# 我们可以指定用于存储和计算的设备，例如CPU或GPU。默认情况下，数据在主内存中创建，然后使用CPU进行计算。\n",
    "# 深度学习框架要求计算的所有输入数据都在同一设备上，无论是CPU还是GPU。\n",
    "# 不经意地移动数据可能会显著降低性能。\n",
    "# 一个典型的错误如下：计算GPU上每个小批量的损失，并在命令行中将其报告给用户（或将其记录在NumPy ndarray中）时，\n",
    "# 将触发全局解释器锁，从而使所有GPU阻塞。最好是为GPU内部的日志分配内存，并且只移动较大的日志。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cpu和gpu计算速度\n",
    "from d2l import torch as d2l\n",
    "time = d2l.Timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.628798246383667"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.start()\n",
    "x = torch.rand(10000, 10000)\n",
    "y = torch.rand(10000, 10000)\n",
    "z = torch.mm(x, y)\n",
    "time.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11199951171875"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.start()\n",
    "x = torch.rand(10000, 10000, device=try_gpu())\n",
    "y = torch.rand(10000, 10000, device=try_gpu())\n",
    "z = torch.mm(x, y)\n",
    "time.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "d2l"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
